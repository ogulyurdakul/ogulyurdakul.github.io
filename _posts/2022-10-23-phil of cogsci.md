---
title: 'Essays on Philosophy of Cognitive Science'
date: 2022-10-30
permalink: /whatnot/2022/phil_of_cogsci/
tags:
  - cognitive science
  - course essays
---

*These are the weekly quizzes for the COGS517 Philosophy of Cognitive Science course I took during the Fall 2022 semester. They are relatively short (2-3 A4 sheets at most), so I decided to gather them in a single post as I write them. They are in chronological order, and as the course progresses they -more or less- follow a certain progression of topics and questions. While they start off heavily philosophy of mind centric, I expect them to turn to philosophy of cognitive science each week.*

1. [Is "autonomous" Same as "automated"?](/whatnot/2022/phil_of_cogsci/#autonomy)
2. [Turing Tests and Chinese Rooms for Other Minds](/whatnot/2022/phil_of_cogsci/#tt_cra_other_minds)

---

## <a id="autonomy"></a> Is "autonomous" Same as "automated"?

First of all, I have to say that for all practical or everyday cases I can imagine, these two words seem to be synonymous: Both kinds of autonomy convey the meaning of a possibly complex process being performed independent of an external agent's interference or control. For sufficiently complex tasks I imagine both entail implementing some sort of auto-correction upon error detection, modelling of the environment and planning on that model, and much more. I have to admit my reluctance in positing the existence of a representation, though. After all, the falling of water at a water flow seems to occur autonomously at face value, while I find it questionable that there is a representation there at all.<sup><a id="footnotemark1_1" href="/whatnot/2022/phil_of_cogsci/#footnotetext1_1">1</a></sup> So, I make no distinction between these two terms in terms of the existence of representation. Both may or may not have it, as far as I am concerned here.

When we probe the two words further and *look* for differences, however, I find that "automated" seems to suggest an "automator," while "autonomous" implies no such agent. Following this distinction, what is automated is inevitably conceptually dependent on its automator, as it is this automator that designs the automated. The input-output relations of the automation process or the internal state transitions are determined by this external agent, who itself must be able to compute and evaluate such decisions beforehand. If the automated has (internal) representations, it is the automator who puts them there. The validity and fitness of these representations are entirely dependent on them, which relieves the automated from accounting for its performance. Even when we are talking about a (machine/reinforcement/artificial) learning agent, what it learns, the rules it finds or the representations it develops cannot go outside the boundaries or the meta-rules its designer imposes. The complexity or performance in behaviour and autonomy of the automated can be considered illusory or misplaced, because it is not really the automated that possesses such qualities, it is the automator.

Putting the "autonomous" at the other side of this distinction, however, makes describing the autonomous significantly more challenging than describing the automated. Firstly, the discussion above seems to remove any sort of designer from the picture of an autonomous. How does the autonomous come to become competent in the complex task it performs? Even the constraints or the well-definition of the input-output relation the autonomous performs can lose their concreteness without such a designer. One way out suggested by  seems to be resorting to natural selection and evolution. According to him, the trial-and-error mechanisms of natural selection and sufficient time (in the order of thousands or millions of years) can yield "competence without comprehension," or autonomy without a higher-level designer. The matter of representation is another problem for the autonomous: If it indeed has internal representations for aspects of its environment and its internal states (meaningful to us or otherwise), how can they come to "mean" what they do, without resorting to the God card or another higher-level being? In other words, autonomous seems a natural subject to the symbol grounding problem from which the automated is exempted, through the existence of its automator.

Lastly, I would like to make a brief discussion on the computer scientist's view over these two kinds of autonomy. Taking Knuth's definition of computer science, i.e. as the study of what can be automated<sup><a id="footnotemark1_2" href="/whatnot/2022/phil_of_cogsci/#footnotetext1_2">2</a></sup> (and I take the word "automated" here to include both kinds of autonomy), I find that the autonomous and the automated alike are subject to the computer scientist's inquiry, although with significant distinctions. Because the automated comes equipped with its automator, the computer scientist's empirical reliance is somewhat relieved. The validity and performance of the algorithm they develop can be contrasted the automator's design criteria directly. However, without assuming the existence of a supernatural designer (like a God) or access to what can be their design process (like heresy), what is automated is almost inevitably a coder's design. This makes the study of the automated by the computer science a search for simplification, refinement or improvement. On the other hand, the solitary nature of the autonomous challenges the empirical needs of the computer scientist: No designer, no access to internal state transition or even the guarantee of them and even no well-defined target problem with any description of desirable outputs to possible inputs. The computer scientist is left to their own devices in narrowing down and solidifying the scope of all of these open questions in the case of the autonomous. The tests of performance of any candidate algorithm/hypothesis must be based on the assumption that the observed inputs and outputs are sufficient in this assessment.

### Notes

1. <a id="footnotetext1_1"></a> I suspect this debate will lead the way to "nature computes" type of arguments, but more in a "nature represents" way. [Back up.](/whatnot/2022/phil_of_cogsci/#footnotemark1_1)
2. <a id="footnotetext1_2"></a> While we talked as if this is Knuth's definition in class, I discovered that in fact Knuth is quoting George E. Forsythe, who wrote about the central questions of computer science. See Knuth (1974, p. 323) for the quote.  [Back up.](/whatnot/2022/phil_of_cogsci/#footnotemark1_2)

*Post-Script Note: I only added the Knuth (1974) reference as I discovered that later.*

### References

Dennett, D. C. (2018). *From Bacteria to Bach and Back: The Evolution of Minds.* Penguin Books.

Knuth, D. E. (1974). Computer Science and its Relation to Mathematics. _The American Mathematical Monthly_, _81_(4), 323-343.

---

## <a id="tt_cra_other_minds"></a> Turing Tests and Chinese Rooms for Other Minds

Before attempting to adapt the Turing Test and the Chinese Room to meaning-bearing minds possibly other than humans', I find it important to clarify what we take these experiments to be. This will allow me and the reader to judge the accuracy of the adaptation and whether the hypothesis tested by the original test is still tested in the adaptation, or what the new hypothesis tested is.

**The Turing Test** is the easier one to deal with among the two, as I find Turing (1950) makes its conditions rather explicit. A human questions a computer (program) and a human subject with the only means of communication with both of them being a textual medium. Both the computer and the human subject try to convince the questioner that they are human and the other is a computer through the answers they give to the questioner. In this summary, while the properties of the communication medium may seem irrelevant to the test, at least the delivery speed of the messages turns out to be of importance: While a computer can multiply large numbers in split seconds, an average human takes significantly longer.<sup><a id="footnotemark2_1" href="/whatnot/2022/phil_of_cogsci/#footnotetext2_1">1</a></sup> Therefore if the two are communicating with pigeon messengers, the questioner will find it impossible to judge how long each side took to perform such a calculation and will not be able to use a criterion otherwise available. In light of this observation, here are the ingredients to a Turing Test:

- *Questioner,* a human who performs the test by asking questions.
- *Questionees,* one human and one computer (program) who are tested via their answers to the Questioner's questions.
- *Communication Medium,* a reliable and fast (let's say with no delay) textual medium that is the only means of passing questions and answers between the Questioner and Questionees.
- *Decision Criterion:* If the Questioner cannot confidently identify the Questionees as human and computer, the computer (program) is said to have passed the Turing test.

**The Chinese Room** thought experiment is more elusive to pin down than the Turing Test is. As pointed out by Hofstadter and Dennett (2010), Searle glosses over important details by appealing to intuition and innocent yet strong suppositions to make his point. However, my goal here is not to discuss the validity of the thought experiment, therefore I will take it as it is given by Searle (1980) with as much fidelity as possible: A human subject $S_1$ who knows language $L_1$ is located in a room with an instruction manual in $L_1$ that contains correlational rules between language $L_1$ and language $L_2$, which the subject does not speak or understand its alphabet. To $S_1$, the letters of $L_2$ are mere squiggles. Another subject $S_2$ who speaks $L_2$ passes written questions into the room. $S_1$ then analyses the written note with the help of the manual and cooks up an answer in $L_2$ that they themselves (supposedly) do not understand. A cornerstone supposition in this setup is that the manual on its own can pass the Turing Test,<sup><a id="footnotemark2_2" href="/whatnot/2022/phil_of_cogsci/#footnotetext2_2">2</a></sup> so it can be taken as the written instructions to an artificial intelligence program; "Strong AI" in Searle's words.  Searle then proceeds with his conclusions, but this is enough for me now to list the following ingredients:

- *Two languages, $L_1$ and $L_2$,* that are significantly minimal overlap (e.g. in their alphabet) to prevent any "linguistic leakage" of meaning.
- *Two subjects, $S_1$ inside the room and $S_2$ outside,* who speak $L_1$ and $L_2$, respectively.<sup><a id="footnotemark2_3" href="/whatnot/2022/phil_of_cogsci/#footnotetext2_3">3</a></sup>
- *Manual that passes the Turing Test,* in $L_1$ that serves to take in questions in $L_2$ and generate an appropriate answer.
- *Communication Medium,* a written, reliable and reasonably fast that is the only means of communication between $S_1$ and $S_2$.

What I found most interesting through the above analysis is that both require one or more language(s) that provide(s) the ground for the evaluation of the test/experiment. A failure case is equivalent to ineptitude in the use of that one language, in comprehension or production. Furthermore, a written form of that language is also required to block unwanted communication. Although the underlying assumption these imply, that is the written language can provide sufficient proof of the adept use of that language in general and hence indicate the existence of a meaning-bearing mind, is rather interesting to me. This is because not even all human languages have written forms, and to what extent the written form of a language can represent its oral form is debatable.<sup><a id="footnotemark2_4" href="/whatnot/2022/phil_of_cogsci/#footnotetext2_4">4</a></sup> More so, a hypothetical mind that does not use *any* language (written and oral alike) to communicate with other members is also outside the boundaries of these thought experiments. That only leaves room for dialects of Mentalese, i.e. languages that are privy to each meaning-bearing yet solitary mind. I find it dubious that such minds can even exist or are discoverable even if they do, so I do not consider that case with comfort. Therefore further on, I will assume that a meaning-bearing mind uses a language in any form (sound waves, electromagnetic waves, etc.) that can be transcribed through a fixed set of written symbols (alphabet). And with these ingredients in mind, I can attempt to adapt the thought experiments to other meaning-bearing minds.

**The Turing Test** then has to be adapted first since the Chinese Room depends on it, and I find it rather easy to adapt. Simply the human is replaced with the other meaning-bearing mind (bird, Martian, etc.), and the pieces work together in the same way as before. Let's take it to be a Martian bird. Then The Martian-Bird Turing Test works as follows: A Questioner Martian Bird Martian-tweets questions that are passed to the Questionees computer and another Martian Bird. The computer and Martian Bird try to convince the Questioner that they are the true Martian Bird and the other is the computer. If the Questioner Martian Bird cannot reliably/confidently decide which is which, the computer passes the Martian Bird Turing test.

While this adaptation was rather easy, it makes no prediction on the possibility of success or failure for different species as it should, just like the original one. That is, even if we knew the Turing Test could be succeeded by a computer programmed to speak an arbitrary human language, we could not tell whether we could have a similar guarantee for another meaning-bearing mind. This implies the following scenario: Let's say we organise a universe-wide Turing Test festival and run the Turing Test (their adaptations) with all the different kinds of meaning-bearing entities from across the universe. Since anything computable can be computed by a Universal Turing Machine, we can comfortably take the computers in all of these tests to be the same hardware of sufficient capacity (memory, computational speed, etc.), although programmed differently to accommodate the necessary linguistic and semantic processing. What if it turns out that with the right computer program per species, one subset of meaning-bearing Questioners consistently identify the computer while others cannot? In other words, what if it turns out that the same hardware that consistently passes the Turing Test for certain kinds of meaning-bearing entities *fails* to do so for others? Would that imply a natural hierarchy between such meaning-bearing minds, say into "Turing-reducible" and "Turing-irreducible"? Would this be a binary classification or a spectrum between the two? I make no claim as to whether this possibility is a viable one, but I find it an interesting one.

**The Chinese Room** is adapted similarly under the assumption that the meaning-bearing minds use transcribable languages: Let's say a Martian Sparrow ($S_1$) that speaks Martian Sparrowese ($L_1$) is located in an isolated cage, together with a manual in Martian Sparrowish that correlates Martian Sparrowish with Martian Pigeonese ($L_2$, which is significantly different from Martian Sparrowish $L_1$) and serves to generate responses in Martian Pigeonese. Similar to our human case, we assume the manual passes the appropriate Turing Test. A Martian Pigeon ($S_2$) then passes written questions into the cage, the Martian Sparrow generates an answer via the manual and passes the answer back. Therefore the Chinese Room in this case becomes a (Martian) Pigeonese Cage quite easily.

The strategy I followed in making these adaptations was keeping the test or experiment species-specific. The goal of the Questioner in the Turing Test is to identify a computer from a subject of the same species who even speaks the same language. The Chinese Room stands differently than the Turing Test in that regard: It relies on the difference in languages and the lack of understanding of the subject in the room caused by it. Therefore I find the idea of a Chinese Room with an Martian Sparrow inside and a Chinese human outside just as reasonable, and it is no different than a Pigeonese Cage with a Martian Pigeon outside and an English person inside. I find the backbone of the thought experiment, that is the fact that the manual passes the appropriate Turing Test, remains intact in all cases. The subsidiary assumption that this manual can be written not in the language it passes the Turing Test but in another seems less critical.

### Notes

1. <a id="footnotetext2_1"></a> Turing mentions the delay can be mimicked by the computer, but I am taking precautions against possible questions that create a difference in reaction times. [Back up.](/whatnot/2022/phil_of_cogsci/#footnotemark2_1)
2. <a id="footnotetext2_2"></a> I admit that I have not read the full target paper and its subsequent discussions. And since I have not seen the passage of Turing Test as a condition of the experiment in Searle's original article, I decided to take the version depicted by Dennett (2016) as a reliable summary. [Back up.](/whatnot/2022/phil_of_cogsci/#footnotemark2_2)
3. <a id="footnotetext2_3"></a> Nothing in principle prevents the outside inquisitor $S_2$ from knowing $L_1$, but let's keep the matters simple. [Back up.](/whatnot/2022/phil_of_cogsci/#footnotemark2_3)
4. <a id="footnotetext2_4"></a> This is important, since if the language cannot be transcribed, both thought experiments lose an important component that is the communication medium. Similarly, a subject that cannot write is useless in these experimental setups, but that is resolved quite easily with a mental reemployment of another subject more suitable for the job. [Back up.](/whatnot/2022/phil_of_cogsci/#footnotemark2_4)

*Post-Script Note: I admit I originally wrote and submitted this one in a rush, so I noticed quite a number of grammar mistakes and typos while posting it here. I only fixed those in the essay. On the other hand, I was also surprised how I took the phrase "meaning-bearing mind" without questioning (it was given in the question text), and I think it is a term worth thinking on. Maybe later.*

### References

Dennett, D. C. (2016). Çince odası. In O. Karakaş (Trans.), *Sezgi Pompaları ve Diğer Düşünme Aletleri* (p. 302). Alfa Yayınları.

Hofstadter, D. R., & Dennett, D. C. (2010). *The Mind's I: Fantasies and Reflections on Self and Soul.* Basic Books.

Searle, J. R. (1980). Minds, brains, and programs. *Behavioral and Brain Sciences, 3* (3), 417-424.

Turing, A. M. (1950, 10). Computing Machinery and Intelligence. *Mind*, LIX(236), 433-460. doi: 10.1093/mind/LIX.236.433